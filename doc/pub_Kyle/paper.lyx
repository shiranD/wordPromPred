#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{spconf}


\ninept

\name{Shiran Dudy and Kyle Gorman}
\address{Center for Spoken Language Understanding, Oregon Health \& Science University\\
Portland, OR, USA\\
{\small \tt dudy@ohsu.edu}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Textual Prediction of Prosodic Prominence in Spontaneous Speech with Sequence
 Classifiers
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Confirmation Number: 1309 Submission Passcode: 1309X-C3A3G6P3E8
\end_layout

\begin_layout Plain Layout
Confirmation Number: 1309 Submission Passcode: 1309X-C3A3G6P3E8 
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Speakers produce words with differing degrees of prosodic prominence, as
 do naturalistic text-to-speech systems.
 Prominence also marks contrasts in information structure.
 We describe models for predicting word prominence using textual features
 (such as part of speech).
 We employ sequence classification techniques so as to encode the tendency
 for prominent and non-prominent words to alternate.
 In experiments with the spontaneous speech from the Switchboard database,
 we show that these models produce a significant improvement in classification
 above the baseline.
 Our results suggest that word-level prominences can be accurately inferred
 from relatively shallow textual features.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
definition
\end_layout

\end_inset

Prominent words in a sentence focuses the listener's attention to a new
 concept presented by the speaker
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "bolinger1986intonation"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "cole2014listening"

\end_inset


\begin_inset space ~
\end_inset

showed that the emphasized words in a sentence shape the meaning of the
 sentence in an audio-listening perceptual test of non-expert, native speakers.
 Furthermore, meaningful prosodic cues supports child language acquisition
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "morgan1987structural"

\end_inset

.
 Monotonic speech is common in children with autism spectrum disorders
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citet
key "shriberg2001speech"

\end_inset

 and has been described as 'machine-like'.
 All this suggests that appropriate use of word prominence is an important
 component of natural and expressive speech.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
TTS+example
\end_layout

\end_inset


\end_layout

\begin_layout Standard
While humans naturally apply prominence, Text to Speech (TTS) systems aim
 to produce speech with appropriate prominences, for at least two reasons.
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "cooke2013evaluating"

\end_inset

.
 First, prominence contributes to the broader meaning of an utterance, as
 the following example shows: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
That's what 
\begin_inset Formula $\text{she}^{<}$
\end_inset

 said
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
That's 
\begin_inset Formula $\text{what}^{<}$
\end_inset

 she said
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
this is the source for stress symbolic icon:
\end_layout

\begin_layout Plain Layout
https://haharoni.wordpress.com/2011/10/11/hatama-02/
\end_layout

\end_inset

Monotonic speech, that contains no prominence, would sound unnatural and
 make the hearer uncertain of the purpose of the utterance.
 However, when `she` is emphasized, in the first example, it is assumed
 that the focus is on her and the information coming next would involve
 additional details to support that fact.
 The same goes with `wha` , given in the second example, only that this
 time the expected details would be on what is being said.
 Secondly, appropriately placed prominences may serve to make synthesized
 speech more natural.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
properties
\end_layout

\end_inset

Where can we find evidence of prominence? At the word level, prominence
 is realized by at least three acoustic features, all defined relative to
 the larger context: a local maxima or minima of the fundamental frequency
 contour, increased duration of the word, and a higher overall amplitude
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "rosenberg2006correlation"

\end_inset

.
 More interestingly, in this work we hypothesize that linguistic features
 contribute to prominence.
 For example, since content words tend to introduce new information, they
 are likely to be accented more often than function words.
 Another example is the presence of contrast in a sentence.
 It is assumed that presenting a clause that undermines a former idea is
 expected to be emphasized in order to point out the contrasting information.One
 key linguistic intuition in the model we propose is the idea that speakers
 tend to avoid ``clashes''---sequences of adjacent prominent words---and
 long ``lapses''--sequences of adjacent non-prominent words.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Another example that influences accent is the presence of a phrase in a
 sentence.
 A phrase is a sequence of accented words within a sentence.
 Within the phrase there will be another dimension of accentuation in the
 form of a word or syllable.
 Another noticeable trend is that the words surrounding a prominent word
 may not, under most circumstances, be prominent
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand citet
key "kimballavoidance"

\end_inset

 find that native speakers have difficulty perceiving ``clashes''.
 We thus employ sequence classification models to model these local dependencies.
 This finding might also suggest that sentence production involves scattering
 prominence to non-prominent environments by avoiding accenting two or more
 units in a row.
\end_layout

\begin_layout Standard
Our goal is to evaluate textual models of word prominence for possible applicati
ons in text-to-speech systems.
 Unlike prior work, which uses read speech, we employ spontaneous speech
 data, which is likely to be more difficult.
 Our final model is far from perfect, but a significant improvement over
 a baseline model.
\end_layout

\begin_layout Standard
Section 2 describes prior work on predicting word prominence.
 Section 3 describes data and features used in our experiments.
 The results of our spontaneous speech experimental results are provided
 in Section 4.
 Section 5 concludes.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Contributions
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
define prominence Cole
\end_layout

\begin_layout Plain Layout
perceptual cues 
\end_layout

\begin_layout Plain Layout
what things that humans hear cause them feel it is prominent
\end_layout

\begin_layout Plain Layout
duration pitch intensity
\end_layout

\begin_layout Plain Layout
information structure - define prominece definitaion 
\end_layout

\begin_layout Plain Layout
why for human why for tts 
\end_layout

\begin_layout Plain Layout
pitch accent Juila hirshcberg
\end_layout

\begin_layout Plain Layout
PROBLEM
\end_layout

\begin_layout Plain Layout
naturalness tts
\end_layout

\begin_layout Plain Layout
poour prominence - 
\end_layout

\begin_layout Plain Layout
natural yet intelligble
\end_layout

\begin_layout Plain Layout
prior work textual 
\end_layout

\begin_layout Plain Layout
intuition lapse 
\end_layout

\begin_layout Plain Layout
clash
\end_layout

\begin_layout Plain Layout
=>sequence clas.
 
\end_layout

\begin_layout Plain Layout
spontaious speech
\end_layout

\end_inset


\end_layout

\begin_layout Section
Prior Work
\end_layout

\begin_layout Standard
The prior work in the field of predicting prominence in a sentence is divided
 to two schools of thought: the research that involves acoustics with or
 without linguistic features and the research that is focused only on linguistic
 features.
 We will review both and point out some interesting methods that were developed.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "rosenberg2009detecting"

\end_inset

studied pitch accent prediction in a variety of domains and found that word
 level led to the highest accuracy predictions; their model achieved 84.2%
 prediction accuracy on read speech.
 Pitch accent is the observed change in pitch that is refereed to as prominence.
 
\begin_inset CommandInset citation
LatexCommand citet
key "rangarajan2007exploiting"

\end_inset

 applied maximum entropy classifiers and a mixture of textual and acoustic
 features for prosody labeling and used Boston University Radio News Corpus
 that scored 86.0% accuracy.
 
\begin_inset CommandInset citation
LatexCommand citet
key "mehrabani2013unsupervised"

\end_inset


\begin_inset space ~
\end_inset

employed clustering in a study of word prominence.
 A study of German incorporated only acoustic features such as nucleus duration,
 spectral emphasis, pitch movements, and intensity to developed a model
 of prominence which was strongly correlated with human perceptual judgements
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "tamburini2007automatic"

\end_inset

.
\end_layout

\begin_layout Standard
In linguistic studies the researchers' focus is on utilizing the syntactic
 and semantic information.
 
\begin_inset CommandInset citation
LatexCommand citet
key "windmann2011prominence"

\end_inset


\begin_inset space ~
\end_inset

developed a rule-based voting system based on features such as part of speech
 tags, and dictionary based stress assignments, to determine the prominence
 assignment.
 Another study 
\begin_inset CommandInset citation
LatexCommand citep
key "brenier2006non"

\end_inset

 suggested that using features such as the frequency of a word, the type
 of a noun, and accent ratio eventually results in a 77% in accuracy on
 spontaneous speech.
 Using features found in Switchboard spontaneous corpus such as contrast
 led to 76.58% in accuracy
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "nenkova2007memorize"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Not convincing enough
\end_layout

\end_inset

In what follows, we employ additional further linguistic features to improve
 prominence prediction for spontaneous speech.
 We proceed to describe the data, features, and machine learning techniques
 used in our experiments.
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
In all our experiments, we used the Switchboard corpus of American English
 spontaneous speech
\begin_inset CommandInset citation
LatexCommand citep
key "godfrey1992switchboard"

\end_inset

 and the accompanying NXT annotation set, which consist of linguistic annotation
s added by other teams after the original Switchboard release.
 The target dataset consists of 40,647 word tokens from 6,425 utterances.
 
\end_layout

\begin_layout Subsection
Features
\end_layout

\begin_layout Standard
We employed the following sets of features:
\end_layout

\begin_layout Itemize
Terminals: the orthographic wordform 
\end_layout

\begin_layout Itemize
POS: the Penn Treebank part of speech tag 
\end_layout

\begin_layout Itemize
CLP-POS: the POS tag mapped onto the 
\begin_inset CommandInset citation
LatexCommand citet
key "petrov2011universal"

\end_inset

 universal part of speech tagset 
\end_layout

\begin_layout Itemize
Function: is the terminal a function word? (in, on, of) 
\end_layout

\begin_layout Itemize
Negation: is the terminal a negation word? (no, not) 
\end_layout

\begin_layout Itemize
Vowels: indicators for each stressed vowel label, assuming left-aligned
 syllables 
\end_layout

\begin_layout Itemize
Nucleus: the vowel label of the nucleus of the primary stressed syllable
 of the terminal 
\end_layout

\begin_layout Itemize
Nucleus type: is the type of nucleus that receives primary stress
\end_layout

\begin_layout Itemize
Kontrast: kontrast level and kontrast type as defined in the database
\end_layout

\begin_layout Itemize
Phrase type: grouping of words in the MS-State transcript into prosodic
 phrases
\end_layout

\begin_layout Itemize
DialAct: the dialogue act description (e.g., 
\begin_inset Quotes eld
\end_inset

question
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

statement
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Standard
Finally, we used as the outcome variable the three prominence levels: `full`,
 `weak`, and `none`.
\end_layout

\begin_layout Standard
Example feature and outcome vectors are provided in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:feature-set"

\end_inset

.
 In the example, only the word 'pretty' was coded as having prominence;
 all other words are `none'.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename featset1.pdf
	lyxscale 40
	scale 35

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:feature-set"

\end_inset

sequence of feature set and its corresponding target accents 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Models
\end_layout

\begin_layout Standard
We employed sequence classification models to predict prominence sequences.
 The baseline model simply guesses the most probable target over the dataset,
 which is 'none'.
 The next three models are variants of a hidden Markov model backed by a
 linear model (the averaged perceptron 
\begin_inset CommandInset citation
LatexCommand citep
key "collins2002discriminative"

\end_inset

).
 The final two models use the PocketCRF toolkit.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://pocket-crf-1.sourceforge.net/
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
We used sequence classifying models to exploit the internal relationships
 found in adjacent observations and consider the problem in its context.
 Models 1-3 were based on Average Perceptron approach 
\begin_inset CommandInset citation
LatexCommand citep
key "collins2002discriminative"

\end_inset

 applying Hidden Markov Models assumptions to these models.
 For models 4, 6 we used a 'PocketCRF' toolkit'
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://pocket-crf-1.sourceforge.net/
\end_layout

\end_inset


\end_layout

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Local Search (
\begin_inset Formula $L$
\end_inset

): the ``zero order'' model -- 
\begin_inset Formula $L_{0}$
\end_inset

-- mode employs only textual features of the current observation (
\begin_inset Formula $X_{t}$
\end_inset

).
 The ``first order'' model -- 
\begin_inset Formula $L_{1}$
\end_inset

-- employs features of the current and preceding observation (
\begin_inset Formula $X_{t}$
\end_inset

, 
\begin_inset Formula $X_{t-1}$
\end_inset

).
 The ``second order'' model -- 
\begin_inset Formula $L_{2}$
\end_inset

-- employs features of the current observation as well as the preceding
 and following observation (
\begin_inset Formula $X_{t-1}$
\end_inset

, 
\begin_inset Formula $X_{t}$
\end_inset

, 
\begin_inset Formula $X_{t+1}$
\end_inset

).
\end_layout

\begin_layout Enumerate
Greedy search (G): The features consist of those extracted from the current
 observation as well as transition features generated from the best hypothesis
 for the preceding prominence label (
\begin_inset Formula $X_{t}$
\end_inset

, 
\begin_inset Formula $\hat{y}_{t-1}$
\end_inset

).
\end_layout

\begin_layout Enumerate
Viterbi search (V): The same as the greedy model, but employing the Viterbi
 algorithm to consider all possible preceding labels 
\begin_inset Formula $\hat{y}_{t-1}$
\end_inset

.
\begin_inset CommandInset citation
LatexCommand citep
key "viterbi1967error"

\end_inset

.
\end_layout

\begin_layout Enumerate
Conditional random fields (CRF): Global inference using a probabilistic
 classifier and the above ``second order'' features.
\begin_inset CommandInset citation
LatexCommand citep
key "lafferty2001conditional"

\end_inset

.
 
\end_layout

\begin_layout Enumerate
Max margin Markov networks (M3N): Global inference using a maximum-margin
 sequence classifier and the above ``second order'' features.
\begin_inset CommandInset citation
LatexCommand citep
key "roller2004max"

\end_inset

.
 
\end_layout

\begin_layout Enumerate
Baseline: our baseline was set by guessing the most probable target found
 in the database.
 In our experiment the guess was 'none' accent type.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
We performed a 10 fold cross-validation test and averaged all accuracy results
 for token and sentence level tests.
\end_layout

\begin_layout Subsection
Token Level Accuracy
\end_layout

\begin_layout Standard
The models were tested for their token accuracy.
 Accuracy was measured by counting the correct classifications over all
 classifications.
 Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:token accuracy"

\end_inset

 describes the accuracy of the classifications using the models.
 We noticed that all 
\begin_inset Formula $L$
\end_inset

 performed the best, then, the Viterbi and the Greedy which were close to
 each other and lasts are the CRF as well as the M3N, yet still seemed to
 be different than the baseline.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="left" valignment="top" width="0pt">
<column alignment="right" valignment="top" width="0pt">
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy[%]
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Baseline
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
69.9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L_{0}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
79.1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
79.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
79.0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
G
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
77.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
V
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
77.5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CRF
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
74.7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
M3N
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
72.3
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:token accuracy"

\end_inset

 Classification Accuracy 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
All models performed better than the baseline but we needed to understand
 how significant results were.
 Next, we applied 
\begin_inset Formula $95\%$
\end_inset

 binomial confidence intervals based on the Wilson score 
\begin_inset CommandInset citation
LatexCommand citep
key "wallis2013binomial"

\end_inset

 to have a better idea of whether the models are different or that their
 confidence interval overlap and they were driven from the same population.
 Looking at Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wilson"

\end_inset

, we noticed that all models seem to differ than the baseline since none
 overlap with it.
 We also saw that 
\begin_inset Formula $L$
\end_inset

 demonstrates overlap within group but does not overlap with Greedy and
 Viterbi.
 Greedy and Viterbi overlap but do not overlap with CRF.
 For the same reason, CRF and M3N do not seem to represent a similar population
 as well.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename wilson.pdf
	lyxscale 40
	scale 40

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:wilson"

\end_inset

Wilson Score 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Since the baseline confidence intervals do not overlap those of any of the
 non-baseline models, we have reason to believe that these models are significan
tly better than the baseline.
 We formalized model comparisons by performing pairwise McNemar tests 
\begin_inset CommandInset citation
LatexCommand citep
key "fagerland2013mcnemar"

\end_inset

 .
 Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:McNeamar"

\end_inset

 shows the results of these comparisons.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Finally, we performed a Mcneamar 
\begin_inset CommandInset citation
LatexCommand citep
key "fagerland2013mcnemar"

\end_inset

 test to determine the significance of our results.
 Mcneamar test compares two models in terms of how many true positive and
 true negtive were predicted in each model -- this determines the differences
 in model populations.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="3">
<features rotate="0" tabularvalignment="middle">
<column alignment="left" valignment="top" width="0pt">
<column alignment="left" valignment="top">
<column alignment="right" valignment="top" width="0pt">
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model A
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model B
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
p-value
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
M3N
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Baseline
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $<0.001$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CRF
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
M3N
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $<0.001$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
G
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CRF
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $<0.001$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
V
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
G
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.020
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
V
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $<0.001$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L_{0}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.050
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L_{0}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.180
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:McNeamar"

\end_inset

 McNemar Significance Test 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:McNeamar"

\end_inset

, we sorted the groups in increasing accuracy order and showed a pairwise
 comparison between two adjacent models.
 P-values that were below 0.001 demonstrated two significantly different
 models.
 We can see that M3N, CRF, Viterbi and Greedy together, and all 
\begin_inset Formula $L$
\end_inset

 form four significantly different populations than the baseline.
 The best model is the 
\begin_inset Formula $L$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Our initial intuition of applying Averaged Perceptron approach was that
 it will perform better since during training the model is penalized proportiona
lly when the prediction is not like the true target.
 This regularization proved to be helpful.
 However, we expected that Viterbi and Greedy search would outperform Local
\end_layout

\end_inset

 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Search since the former incorporate knowledge of the past prediction 
\begin_inset Formula $\hat{y}(t-1)$
\end_inset

 and it thought to be an important feature for deciding whether the current
 feature should be prominent.
 The possible explanation for that might derive from the transition features
 that were added to the input in Viterbi and Greedy and damaged the learning.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We initially expected that the Greedy and Viterbi models would outperform
 local classification, as the expectations about the prior labels would
 prove to be predictive of the current label.
 However, these features appear to actually result in slightly less accurate
 models.
\end_layout

\begin_layout Subsection
Sentence Level Accuracy 
\end_layout

\begin_layout Standard
As pointed out earlier, even a single erroneous assignment of prominence
 may greatly affect the meaning of a sentence, and may also lead human listeners
 to judge the sentence as unnatural.
 Therefore, we adopted a more stringent measure of accuracy that required
 entire sentences to be correctly labeled.
 This is the one case where we can be confident that the prominence system
 is conveying a naturalistic message.
 Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:sentence accuracy"

\end_inset

 provides sentence-level accuracy results.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="left" valignment="top" width="0pt">
<column alignment="right" valignment="top" width="0pt">
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accuracy[%]
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Baseline
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
42.0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
L 0th
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
48.4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
L 1st
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
48.9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
L 2nd
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
48.9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
G
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
48.9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
V
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
48.9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CRF
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
34.8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
M3N
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
32.8
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:sentence accuracy"

\end_inset

 Classification Accuracy 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
To finalize these results we used a Binomial Test to measure significance.
 We compared accuracies of baseline with all models that were above baseline.
 We conducted a Binomial Test that compares between two models' accuracy
 result and found that the Local Search 0 order is significantly different
 than the baseline with p-value of 5.19e-75.
 The p-value for Viterbi, Greedy, Local Search 1 and 2 with the baseline
 is 2.22e-84.
 Our models, therefore, were able to send a more correct messages than using
 baseline predictions.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:sentence accuracy"

\end_inset

 shows that the Local, Greedy and Viterbi Search share similar pattern learning
 that is expressed in sentence accuracy.
 We can also infer that though the Local Search has a better accuracy result
 on a token level it is not expressed in terms of sentence accuracy.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this case, the baseline is defined as by counting how many fully correct
 sentences are predicted by using a a model the guesses the common label
 in the corpus.
 According to the Binomial test, the local classifiers, greedy search, and
 Viterbi search are all significant improvements on the baseline.
 It is interesting to note that while the local classifiers were superior
 at the token level, their performance at the sentence level is matched
 by the greedy and Viterbi search.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
We showed that simple textual features extracted from spontaneous speech
 can be used to predict human-like word prominence sequences, and that these
 features, when exploited by modern sequence classification models, produce
 a prominence placement model that overperforms the baseline.
 This knowledge may thus result in better---more intelligible and more natural--
-text-to-speech.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "prom"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document
